{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68cc2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import ChessBoardTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276b65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "\n",
      "Output Embedded Board Shape: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "tokenizer = ChessBoardTokenizer(emb_dim=EMBEDDING_DIM)\n",
    "\n",
    "# Starting position FEN\n",
    "fen_start = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "\n",
    "print(f\"Tokenizing FEN: {fen_start}\")\n",
    "\n",
    "# Generate the embedded board\n",
    "board_emb = tokenizer(fen_start)\n",
    "\n",
    "print(f\"\\nOutput Embedded Board Shape: {board_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea3ac276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219a50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized. Vocabulary Size: 4672\n",
      "-----------------------------------\n",
      "Encoding e2e4 â†’ ID 898\n"
     ]
    }
   ],
   "source": [
    "from AlphaZeroEncoder import AlphaZeroMoveEncoder\n",
    "\n",
    "encoder = AlphaZeroMoveEncoder()\n",
    "print(f\"Encoder initialized. Vocabulary Size: {encoder.VOCAB_SIZE}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# 1. Encode Example (Normal Move)\n",
    "move_uci_1 = \"e2e4\"\n",
    "move_id_1 = encoder.encode(move_uci_1)\n",
    "print(f\"Encoding {move_uci_1} â†’ ID {move_id_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef4dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4    â†’ e2e4\n",
      "Encoded ID: 898\n",
      "Nf3   â†’ g1f3\n",
      "Encoded ID: 474\n",
      "d4    â†’ d2d4\n",
      "Encoded ID: 825\n",
      "c4    â†’ c2c4\n",
      "Encoded ID: 752\n"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "\n",
    "def algebraic_to_uci(board_fen, algebraic_move):\n",
    "    \"\"\"\n",
    "    Convert Stockfish-style algebraic notation (e.g., 'Nf3', 'exd5')\n",
    "    into UCI notation (e.g., 'g1f3', 'e4d5').\n",
    "    \"\"\"\n",
    "    board = chess.Board(board_fen)\n",
    "    move = board.parse_san(algebraic_move)  # convert algebraic â†’ Move\n",
    "    return move.uci()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "moves = [\"e4\", \"Nf3\", \"d4\", \"c4\", ]\n",
    "\n",
    "for algebraic in moves:\n",
    "    uci = algebraic_to_uci(fen, algebraic)\n",
    "    print(f\"{algebraic:5} â†’ {uci}\")\n",
    "    print(f\"Encoded ID: {encoder.encode(uci)}\")\n",
    "    # Update the board for the next move if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be417b3f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c6082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating FENs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1994.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def random_fen():\n",
    "    board = chess.Board()\n",
    "    for _ in range(random.randint(0, 20)):\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "        move = random.choice(list(board.legal_moves))\n",
    "        board.push(move)\n",
    "    return board.fen()\n",
    "\n",
    "def legal_moves(fen):\n",
    "    board = chess.Board(fen)\n",
    "    return [board.san(m) for m in board.legal_moves]\n",
    "def create_random_dataset(n=100):\n",
    "    data = []\n",
    "    for _ in tqdm(range(n), desc=\"Generating FENs\"):\n",
    "        fen = random_fen()\n",
    "        moves = legal_moves(fen)\n",
    "        for move in moves:\n",
    "            # moves_str = \" \".join(sorted(move))  # ðŸ‘ˆ sort for order-invariance\n",
    "            data.append({\"input\": fen, \"output\": move})\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset = create_random_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c374a1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb7fa9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ng5+'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5951177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uci = algebraic_to_uci(dataset['input'][0], \"Ng5+\")\n",
    "encoder.encode(uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ccfc257",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (128) at non-singleton dimension 0.  Target sizes: [2].  Tensor sizes: [128]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChessGPT\n\u001b[32m      2\u001b[39m model = ChessGPT()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m logit = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard_emb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversityofWyoming/2025/Fall/Classes/LLM/Deep-Evolve/model.py:75\u001b[39m, in \u001b[36mChessGPT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     73\u001b[39m B = x.size(\u001b[32m0\u001b[39m)\n\u001b[32m     74\u001b[39m x = x.view(B, \u001b[32m64\u001b[39m, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# flatten board\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m     78\u001b[39m     x = layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversityofWyoming/2025/Fall/Classes/LLM/Deep-Evolve/model.py:31\u001b[39m, in \u001b[36mPositionalEmbedding2D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BOARD_SIZE):\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BOARD_SIZE):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m         \u001b[43mpos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m = \u001b[38;5;28mself\u001b[39m.row_embed[i] + \u001b[38;5;28mself\u001b[39m.col_embed[j]\n\u001b[32m     32\u001b[39m pos = pos.view(\u001b[32m1\u001b[39m, \u001b[32m64\u001b[39m, D)  \u001b[38;5;66;03m# (1, 64, emb_dim)\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x + pos\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (2) must match the existing size (128) at non-singleton dimension 0.  Target sizes: [2].  Tensor sizes: [128]"
     ]
    }
   ],
   "source": [
    "from model import ChessGPT\n",
    "model = ChessGPT()\n",
    "logit = model(board_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec02a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4672])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ff00f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e03eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputs = []\n",
    "Outputs = []\n",
    "for i in range(len(dataset)):\n",
    "    Inputs.append(tokenizer(dataset['input'][i]))\n",
    "    Outputs.append(encoder.encode(algebraic_to_uci(dataset['input'][i], dataset['output'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039043bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "my_dataset = MyDataset(Inputs, Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2268e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(my_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea014a72",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m logits = model(inputs)              \u001b[38;5;66;03m# 2. Forward pass\u001b[39;00m\n\u001b[32m     19\u001b[39m loss = torch.nn.functional.cross_entropy(logits, targets)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                     \u001b[38;5;66;03m# 3. Backward pass (once per batch)\u001b[39;00m\n\u001b[32m     22\u001b[39m optimizer.step()                    \u001b[38;5;66;03m# 4. Update model\u001b[39;00m\n\u001b[32m     24\u001b[39m total_loss += loss.item()           \u001b[38;5;66;03m# Log scalar, not tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/Deep-Evolve/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()               # 1. Clear old gradients\n",
    "\n",
    "        logits = model(inputs)              # 2. Forward pass\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        loss.backward()                     # 3. Backward pass (once per batch)\n",
    "        optimizer.step()                    # 4. Update model\n",
    "\n",
    "        total_loss += loss.item()           # Log scalar, not tensor\n",
    "\n",
    "        # Print progress occasionally\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Batch [{batch_idx+1}/{len(dataloader)}] | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29304a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positions captured: 34\n",
      "------------------------------\n",
      "First 5 FENs:\n",
      "Move 0: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "Move 1: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1\n",
      "Move 2: rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2\n",
      "Move 3: rnbqkbnr/pppp1ppp/8/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2\n",
      "Move 4: rnbqkbnr/ppp2ppp/3p4/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 0 3\n",
      "...\n",
      "Last FEN (Checkmate):\n",
      "1n1Rkb1r/p4ppp/4q3/4p1B1/4P3/8/PPP2PPP/2K5 b k - 1 17\n"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "from io import StringIO\n",
    "from typing import List\n",
    "\n",
    "def pgn_to_fen_sequence(pgn_string: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Converts a PGN (Portable Game Notation) string containing one game\n",
    "    into a list of FEN (Forsyth-Edwards Notation) strings, representing\n",
    "    every position in the game.\n",
    "\n",
    "    Args:\n",
    "        pgn_string: A string containing the full PGN of a single chess game.\n",
    "\n",
    "    Returns:\n",
    "        A list of FEN strings, starting with the initial position (rnbq.../ w).\n",
    "        Returns an empty list if the PGN cannot be parsed.\n",
    "    \"\"\"\n",
    "    # Use StringIO to treat the PGN string like a file for the chess.pgn parser\n",
    "    pgn_io = StringIO(pgn_string)\n",
    "    \n",
    "    # Load the game from the PGN file-like object\n",
    "    game = chess.pgn.read_game(pgn_io)\n",
    "    \n",
    "    if game is None:\n",
    "        print(\"Error: Could not parse the PGN string.\")\n",
    "        return []\n",
    "\n",
    "    fen_list = []\n",
    "    \n",
    "    # Start with the initial position of the game\n",
    "    board = game.board()\n",
    "    \n",
    "    # 1. Add the starting position FEN (usually the standard starting position)\n",
    "    fen_list.append(board.fen())\n",
    "\n",
    "    # Iterate through every move in the game's main variation\n",
    "    # 'node' starts at the root of the game tree\n",
    "    for move in game.mainline_moves():\n",
    "        # Apply the move to the board object\n",
    "        board.push(move)\n",
    "        \n",
    "        # Record the resulting FEN string\n",
    "        fen_list.append(board.fen())\n",
    "        \n",
    "    return fen_list\n",
    "\n",
    "# === Example Usage ===\n",
    "\n",
    "# A famous short game: Paul Morphy vs. The Duke and Count (Opera Game)\n",
    "# Note: The result (1-0) is important for the parser.\n",
    "opera_game_pgn = \"\"\"\n",
    "[Event \"Paris Opera\"]\n",
    "[Site \"Paris FRA\"]\n",
    "[Date \"1858.00.00\"]\n",
    "[Round \"-\"]\n",
    "[White \"Morphy, Paul\"]\n",
    "[Black \"Duke Karl / Count Isouard\"]\n",
    "[Result \"1-0\"]\n",
    "\n",
    "1. e4 e5 2. Nf3 d6 3. d4 Bg4 4. dxe5 Bxf3 5. Qxf3 dxe5 6. Bc4 Nf6 7. Qb3 Qe7 \n",
    "8. Nc3 c6 9. Bg5 b5 10. Nxb5 cxb5 11. Bxb5+ Nbd7 12. O-O-O Rd8 13. Rxd7 Rxd7 \n",
    "14. Rd1 Qe6 15. Bxd7+ Nxd7 16. Qb8+ Nxb8 17. Rd8# 1-0\n",
    "\"\"\"\n",
    "\n",
    "# Get the list of FENs\n",
    "fen_sequence = pgn_to_fen_sequence(opera_game_pgn)\n",
    "\n",
    "print(f\"Total positions captured: {len(fen_sequence)}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"First 5 FENs:\")\n",
    "for i, fen in enumerate(fen_sequence[:5]):\n",
    "    print(f\"Move {i}: {fen}\")\n",
    "print(\"...\")\n",
    "print(\"Last FEN (Checkmate):\")\n",
    "print(fen_sequence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6664993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Evolve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
